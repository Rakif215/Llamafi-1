{
    "model_name": "NousResearch/Llama-2-7b-chat-hf",
    "dataset_name": "mlabonne/guanaco-llama2-1k",
    "new_model": "Llama-2-7b-chat-finetune",
    "QLoRA_parameters": {
      "lora_r": 64,
      "lora_alpha": 16,
      "lora_dropout": 0.1
    },
    "bitsandbytes_parameters": {
      "use_4bit": true,
      "bnb_4bit_compute_dtype": "float16",
      "bnb_4bit_quant_type": "nf4",
      "use_nested_quant": false
    },
    "TrainingArguments_parameters": {
      "output_dir": "./results",
      "num_train_epochs": 1,
      "fp16": false,
      "bf16": false,
      "per_device_train_batch_size": 4,
      "per_device_eval_batch_size": 4,
      "gradient_accumulation_steps": 1,
      "gradient_checkpointing": true,
      "max_grad_norm": 0.3,
      "learning_rate": 0.0002,
      "weight_decay": 0.001,
      "optim": "paged_adamw_32bit",
      "lr_scheduler_type": "cosine",
      "max_steps": -1,
      "warmup_ratio": 0.03,
      "group_by_length": true,
      "save_steps": 0,
      "logging_steps": 25
    },
    "SFT_parameters": {
      "max_seq_length": null,
      "packing": false,
      "device_map": {
        "": 0
      }
    }
  }
  